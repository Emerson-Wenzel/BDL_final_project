{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bnn import BNNBayesbyBackprop, gauss_logpdf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/smallTrainCleaned.csv')\n",
    "y = pd.read_csv('data/y_labels.csv', header=None)\n",
    "y[y == -1] = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, test_size=.20)\n",
    "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "y_train, y_test = y_train.to_numpy(), y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data being saved in following file:\n",
      "logging.csv\n",
      "Epoch:  0 \tLoss:  316324.34 \tacc:  0.4904\n",
      "Epoch:  1 \tLoss:  294325.7 \tacc:  0.1199\n",
      "Epoch:  2 \tLoss:  300157.0 \tacc:  0.59295\n",
      "Epoch:  3 \tLoss:  290549.06 \tacc:  0.730975\n",
      "Epoch:  4 \tLoss:  278995.2 \tacc:  0.9023\n",
      "Epoch:  5 \tLoss:  274816.3 \tacc:  0.962625\n",
      "Epoch:  6 \tLoss:  259880.5 \tacc:  0.409225\n",
      "Epoch:  7 \tLoss:  232963.05 \tacc:  0.98075\n",
      "Epoch:  8 \tLoss:  237434.06 \tacc:  0.89765\n",
      "Epoch:  9 \tLoss:  224855.1 \tacc:  0.13035\n",
      "Epoch:  10 \tLoss:  205353.52 \tacc:  0.85545\n",
      "Epoch:  11 \tLoss:  180097.22 \tacc:  0.786625\n",
      "Epoch:  12 \tLoss:  154698.19 \tacc:  0.97175\n",
      "Epoch:  13 \tLoss:  140213.22 \tacc:  0.80455\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-0cc5e3504a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBNNBayesbyBackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_MC_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Tufts_Masters/Semester_3/Bayesian Deep Learning/BDL_final_project/bnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, learning_rate, n_epochs, batch_size, plot)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start_i\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_end_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start_i\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_end_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMC_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                 \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tufts_Masters/Semester_3/Bayesian Deep Learning/BDL_final_project/bnn.py\u001b[0m in \u001b[0;36mMC_elbo\u001b[0;34m(self, X_ND, y_N, curr_batch, n_batches, epoch)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregate_log_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_log_post_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_log_likeli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_log_s_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_MC_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mnn_output_Nx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mnn_output_mu_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_output_Nx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mnn_output_log_s_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_output_Nx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tufts_Masters/Semester_3/Bayesian Deep Learning/BDL_final_project/bnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_ND, predict, num_preds)\u001b[0m\n\u001b[1;32m    153\u001b[0m               \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m               \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tufts_Masters/Semester_3/Bayesian Deep Learning/BDL_final_project/bnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_ND, predict, num_preds)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_DO\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_O\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# print(\"output shape: \", output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             self.log_prior = (gauss_logpdf(W_DO, self.prior_mu, self.prior_s).sum() + \n\u001b[0m\u001b[1;32m    112\u001b[0m                               gauss_logpdf(b_O, self.prior_mu, self.prior_s).sum()) \n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tufts_Masters/Semester_3/Bayesian Deep Learning/BDL_final_project/bnn.py\u001b[0m in \u001b[0;36mgauss_logpdf\u001b[0;34m(x, mu, s)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exploded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnormalized_x\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bnn = BNNBayesbyBackprop(prior_mu=0.0, prior_s=1.0, num_MC_samples=100, linear_regression=True, preset=False, classification=True, input_dim=38)\n",
    "bnn.fit(X_train, y_train, plot=True, n_epochs=75, learning_rate=1e-2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_train).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_train[(y_train != 0) & (y_train != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/Pytorch/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4789"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021466565349544074"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lr.predict(X_test)\n",
    "# confusion_matrix(y_test, preds)\n",
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.653179190751445"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01760455886842373"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test == 1].shape[0] / y_test[y_test == 0].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4676, 5151],\n",
       "       [  60,  113]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(y).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49110,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y ==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy[:,0] > 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.4612076  -7.98423436]\n",
      " [ 9.36505044 -6.60489122]\n",
      " [ 9.5102388   8.35842172]\n",
      " [ 9.61898495  3.42591915]\n",
      " [ 9.71026683 -5.67810617]\n",
      " [ 9.23841086  6.51317567]\n",
      " [ 9.42640497 -1.0176572 ]\n",
      " [ 9.7857565  -0.52367214]\n",
      " [ 9.92158436  4.28678326]\n",
      " [ 9.13796458 -4.89280723]\n",
      " [ 9.89988782  3.08963105]\n",
      " [ 9.17622911  4.33702406]\n",
      " [ 9.35171783  2.10345286]\n",
      " [ 9.44971082  4.97959766]\n",
      " [ 9.71842134  2.71896651]\n",
      " [ 9.74794055  2.97106655]\n",
      " [ 9.51665512 -2.73105866]\n",
      " [ 9.49767072  8.48430903]\n",
      " [ 9.31638517 -2.21039005]\n",
      " [ 9.62894365 -5.40867976]\n",
      " [ 9.40587597 -6.58062797]\n",
      " [ 9.00698142  3.93684372]\n",
      " [ 9.61572032  2.07215793]\n",
      " [ 9.4343037   0.26925859]\n",
      " [ 9.3157381  -0.98626546]\n",
      " [ 9.97681496  1.18294783]\n",
      " [ 9.04432775 -6.65964969]\n",
      " [ 9.61914225 -4.78708475]\n",
      " [ 9.3098394   5.39033509]\n",
      " [ 9.56394897 -4.99675407]\n",
      " [ 9.19844038  1.50908014]\n",
      " [ 9.59415166  5.20886938]\n",
      " [ 9.1905281   7.95289821]\n",
      " [ 9.61958961 -6.91916836]\n",
      " [ 9.39704071 -3.5281518 ]\n",
      " [ 9.22342435 -6.59211486]\n",
      " [ 9.15403002 -3.5880211 ]\n",
      " [ 9.55203619  2.35530881]\n",
      " [ 9.7836793   1.42017247]\n",
      " [ 9.78707234 -7.01435299]\n",
      " [ 9.27586185  8.57980308]\n",
      " [ 9.41995622 -3.62275214]\n",
      " [ 9.91044763  4.98876899]\n",
      " [ 9.98548888  1.08106022]\n",
      " [ 9.30865162  5.41022976]\n",
      " [ 9.12455569  2.5922908 ]\n",
      " [ 9.73378966 -2.48973512]\n",
      " [ 9.34253202  8.38793436]\n",
      " [ 9.04004138  9.40288906]\n",
      " [ 9.20622773 -8.66676263]\n",
      " [ 9.56152087 -9.11945571]\n",
      " [ 9.3407741  -7.75815965]\n",
      " [ 9.60179466  3.91661745]\n",
      " [ 9.55255911 -1.12690226]\n",
      " [ 9.58760383 -1.48160025]\n",
      " [ 9.36697432 -9.75495745]\n",
      " [ 9.93669313  9.89862369]\n",
      " [ 9.67580784  7.69960095]\n",
      " [ 9.50141668  1.27667567]\n",
      " [ 9.42400092  1.78895667]\n",
      " [ 9.25790254  0.52852806]\n",
      " [ 9.89101876  9.59776967]\n",
      " [ 9.19986754  9.88775158]\n",
      " [ 9.42773916 -5.82880563]\n",
      " [ 9.098325    1.40960611]\n",
      " [ 9.23847431  8.39171473]\n",
      " [ 9.8456684  -4.6926832 ]\n",
      " [ 9.07392694  2.80827292]\n",
      " [ 9.73882242  1.4775698 ]\n",
      " [ 9.09181791  2.81114174]\n",
      " [ 9.32803336 -4.51733434]\n",
      " [ 9.42434737  9.91292792]\n",
      " [ 9.17334223  1.61397363]\n",
      " [ 9.27462038  9.6330036 ]\n",
      " [ 9.81751108  9.58469371]\n",
      " [ 9.103628   -9.86708032]\n",
      " [ 9.09174683 -4.3187057 ]\n",
      " [ 9.14357579  7.66086241]\n",
      " [ 9.77193308 -4.43398923]\n",
      " [ 9.53499     6.20280345]\n",
      " [ 9.28764567  9.20219142]\n",
      " [ 9.39568914  9.34446341]\n",
      " [ 9.33550506 -3.14124782]\n",
      " [ 9.03122074 -1.56962813]\n",
      " [ 9.86883114  3.93977783]\n",
      " [ 9.98059602 -6.2535182 ]\n",
      " [ 9.67779098  0.43617074]\n",
      " [ 9.6716116  -3.61275476]\n",
      " [ 9.4238956  -0.31045725]\n",
      " [ 9.18822254  9.04600308]\n",
      " [ 9.09298492 -3.69838375]\n",
      " [ 9.65977958 -4.55617967]\n",
      " [ 9.83393571 -8.01623763]\n",
      " [ 9.73293636  2.10532366]\n",
      " [ 9.40438806 -3.62040555]\n",
      " [ 9.91199993  6.40147662]\n",
      " [ 9.88186793  8.26043124]\n",
      " [ 9.70757142 -2.49003926]\n",
      " [ 9.98791661  7.77909976]\n",
      " [ 9.06877358 -0.52744349]\n",
      " [ 9.03706779 -2.13570732]\n",
      " [ 9.06390262 -8.44241472]\n",
      " [ 9.13656311  0.6277603 ]\n",
      " [ 9.00674202  4.00810116]\n",
      " [ 9.69882872  3.46927132]\n",
      " [ 9.73526695  7.60981446]\n",
      " [ 9.9054412  -8.18470336]\n",
      " [ 9.81763717  4.22824226]\n",
      " [ 9.58152867 -0.77093953]\n",
      " [ 9.50216685 -3.34936458]\n",
      " [ 9.09589967 -6.62356976]\n",
      " [ 9.76650017 -1.96365003]\n",
      " [ 9.12615978  0.44150809]\n",
      " [ 9.74322406  2.57484286]\n",
      " [ 9.20330346  2.02410212]\n",
      " [ 9.61142798 -8.9576815 ]\n",
      " [ 9.53454303  5.74141986]\n",
      " [ 9.09646015  4.0883161 ]\n",
      " [ 9.32414196 -4.81462009]\n",
      " [ 9.76167741 -6.83413512]\n",
      " [ 9.25374338 -8.27489657]\n",
      " [ 9.60074734  5.52115999]\n",
      " [ 9.09404651 -8.17181483]\n",
      " [ 9.64501627 -7.67628068]\n",
      " [ 9.27881555  4.92175943]\n",
      " [ 9.65083604 -3.45398081]\n",
      " [ 9.15283834  3.83349399]\n",
      " [ 9.97814677 -2.92319877]\n",
      " [ 9.2666622   3.47797157]\n",
      " [ 9.45869885 -9.57538169]\n",
      " [ 9.08625808 -3.22497808]\n",
      " [ 9.80660109  4.79758465]\n",
      " [ 9.0805635  -7.59753423]\n",
      " [ 9.21456022 -1.44942545]\n",
      " [ 9.84928228  7.82046893]\n",
      " [ 9.52622926  6.7156531 ]\n",
      " [ 9.47576596  2.88365784]\n",
      " [ 9.99749932 -4.02787965]\n",
      " [ 9.65445059  2.04706499]\n",
      " [ 9.2294271   3.72292961]\n",
      " [ 9.87334682  6.76436154]\n",
      " [ 9.79114584  2.90341918]\n",
      " [ 9.12388127  3.48002258]\n",
      " [ 9.70923718  7.34520111]\n",
      " [ 9.0908353   7.48534826]\n",
      " [ 9.64311688 -6.79538936]\n",
      " [ 9.61641406  9.58306992]\n",
      " [ 9.98009727  6.67304243]\n",
      " [ 9.74390679  9.22735448]\n",
      " [ 9.30007753 -2.78443403]\n",
      " [ 9.49326466  1.62725221]\n",
      " [ 9.24279927 -5.85478371]\n",
      " [ 9.72481306  8.75819758]\n",
      " [ 9.07816098 -7.11423541]\n",
      " [ 9.38002637 -8.67723347]\n",
      " [ 9.12303321  4.64090624]\n",
      " [ 9.44508195  5.01306326]\n",
      " [ 9.53177589 -3.06223914]\n",
      " [ 9.29921454 -0.61084795]\n",
      " [ 9.19094255 -5.51690038]\n",
      " [ 9.21367639 -2.0148099 ]\n",
      " [ 9.61025124 -5.91556261]\n",
      " [ 9.10712345  3.63371238]\n",
      " [ 9.92464701 -4.47873493]\n",
      " [ 9.3836965   4.33583784]\n",
      " [ 9.47173312  3.02331724]\n",
      " [ 9.43310646 -6.79075745]\n",
      " [ 9.91248831 -3.22081017]\n",
      " [ 9.85920727  8.25592735]\n",
      " [ 9.03450431 -4.50358861]\n",
      " [ 9.04287586  9.12856567]\n",
      " [ 9.28488283  3.34506141]\n",
      " [ 9.63971886  3.08070526]\n",
      " [ 9.43641849  2.31679633]\n",
      " [ 9.46363881  8.05774839]\n",
      " [ 9.46727224 -7.68428166]\n",
      " [ 9.38483249  3.40468118]\n",
      " [ 9.85090956  3.60133681]\n",
      " [ 9.45968937 -7.26190555]\n",
      " [ 9.1122198  -7.66348231]\n",
      " [ 9.94516082 -2.58274766]\n",
      " [ 9.95173409 -6.33155818]\n",
      " [ 9.16631322  5.28551342]\n",
      " [ 9.89290057  1.93107059]\n",
      " [ 9.17471695 -8.63468587]\n",
      " [ 9.97304761  0.13737421]\n",
      " [ 9.20779569 -2.05223176]\n",
      " [ 9.34081499  6.74393598]\n",
      " [ 9.91841643 -1.89490425]\n",
      " [ 9.23927032 -0.06610657]\n",
      " [ 9.49205253  7.70283406]\n",
      " [ 9.10016178 -8.21095694]\n",
      " [ 9.87097145 -6.50551358]\n",
      " [ 9.58432046 -9.84491561]\n",
      " [ 9.70717955 -3.68504694]\n",
      " [ 9.1271897   1.09599433]\n",
      " [ 9.17915517 -4.25612467]\n",
      " [ 9.95234269 -9.62797729]\n",
      " [ 9.69355308 -9.67861768]\n",
      " [ 9.57710388  0.84660968]\n",
      " [ 9.70094469 -3.96160403]\n",
      " [ 9.8874342   9.64471598]\n",
      " [ 9.81040297 -2.58061794]\n",
      " [ 9.21029314  8.64654577]\n",
      " [ 9.87047298 -7.19750905]\n",
      " [ 9.61145892  7.10344463]\n",
      " [ 9.89778903  3.29892302]\n",
      " [ 9.59158273 -3.7623364 ]\n",
      " [ 9.92448031 -8.54141178]\n",
      " [ 9.99682754  0.61006076]\n",
      " [ 9.73927828  3.63986665]\n",
      " [ 9.02531632 -0.67417708]\n",
      " [ 9.73117513  7.63464012]\n",
      " [ 9.68949702  5.95830707]\n",
      " [ 9.29461889 -0.83772947]\n",
      " [ 9.25627414 -5.73111842]\n",
      " [ 9.83954996  4.24494959]\n",
      " [ 9.7826718   4.63579918]\n",
      " [ 9.10004841  8.59689124]\n",
      " [ 9.30926969 -5.09234558]\n",
      " [ 9.61845576  7.07644786]\n",
      " [ 9.46867067 -2.30714418]\n",
      " [ 9.38056719  8.97800839]\n",
      " [ 9.79905863 -3.60799733]\n",
      " [ 9.97441731 -7.71322276]\n",
      " [ 9.57555526 -1.41725417]\n",
      " [ 9.90264994  7.42318517]\n",
      " [ 9.83850877 -5.55804804]\n",
      " [ 9.02802019  3.51067805]\n",
      " [ 9.10927771  9.16016529]\n",
      " [ 9.30357078 -6.50779086]\n",
      " [ 9.21917095  1.24533813]\n",
      " [ 9.8001935   5.84612609]\n",
      " [ 9.50441355 -9.32222106]\n",
      " [ 9.14750294  7.10425119]\n",
      " [ 9.3078915  -5.45777196]\n",
      " [ 9.57378008  6.36064461]\n",
      " [ 9.9218572  -6.73985465]\n",
      " [ 9.94480815  6.61302529]\n",
      " [ 9.16783819 -8.49051717]\n",
      " [ 9.68656127  6.24031313]\n",
      " [ 9.29109349 -0.17233886]\n",
      " [ 9.02337422  3.58345417]\n",
      " [ 9.02945445  4.56893137]\n",
      " [ 9.98307908 -1.58655735]\n",
      " [ 9.26839535  5.85896225]\n",
      " [ 9.64763019  3.24355054]\n",
      " [ 9.60749277 -2.58904935]\n",
      " [ 9.55591179  8.48481614]\n",
      " [ 9.75026603  4.74296711]\n",
      " [ 9.5222165   9.66122952]\n",
      " [ 9.91001024  3.22701899]\n",
      " [ 9.018676   -9.5649336 ]\n",
      " [ 9.07745697 -2.66367921]\n",
      " [ 9.60448513 -8.79256462]\n",
      " [ 9.61468488 -9.00986471]\n",
      " [ 9.21445363  7.93369066]\n",
      " [ 9.6662433   3.46597184]\n",
      " [ 9.37824745 -5.52795659]\n",
      " [ 9.46066455  4.24752788]\n",
      " [ 9.20720766 -4.4159478 ]\n",
      " [ 9.28887562 -7.70049237]\n",
      " [ 9.1770174  -4.39810959]\n",
      " [ 9.68493604 -7.81412095]\n",
      " [ 9.62552392  2.47182189]\n",
      " [ 9.7497527   6.05165855]]\n",
      "[0. 0. 1. ... 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_toy = np.random.uniform(-10, 10, (5000, 2))\n",
    "y_toy = np.zeros(X_toy.shape[0])\n",
    "y_toy[X_toy[:,0] > 0] = 1\n",
    "print(X_toy[X_toy[:,0] > 9])\n",
    "print(y_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  2\n",
      "Data being saved in following file:\n",
      "logging.csv\n",
      "Epoch:  0 \tLoss:  14971431.0 \tMAE:  3.0426614\n",
      "Epoch:  1 \tLoss:  17160134.0 \tMAE:  8.388664\n",
      "Epoch:  2 \tLoss:  15540844.0 \tMAE:  9.913533\n",
      "Epoch:  3 \tLoss:  16641569.0 \tMAE:  4.9475875\n",
      "Epoch:  4 \tLoss:  12625564.0 \tMAE:  4.3398924\n",
      "Epoch:  5 \tLoss:  16091138.0 \tMAE:  4.2442455\n",
      "Epoch:  6 \tLoss:  14443296.0 \tMAE:  7.0217314\n",
      "Epoch:  7 \tLoss:  14533060.0 \tMAE:  4.956892\n",
      "Epoch:  8 \tLoss:  13907706.0 \tMAE:  7.6038423\n",
      "Epoch:  9 \tLoss:  14948668.0 \tMAE:  9.094907\n",
      "Epoch:  10 \tLoss:  16398707.0 \tMAE:  4.697731\n",
      "Epoch:  11 \tLoss:  14351789.0 \tMAE:  4.581734\n",
      "Epoch:  12 \tLoss:  13030878.0 \tMAE:  5.277932\n",
      "Epoch:  13 \tLoss:  13720167.0 \tMAE:  10.914837\n",
      "Epoch:  14 \tLoss:  14272938.0 \tMAE:  3.3739114\n",
      "Epoch:  15 \tLoss:  458765300.0 \tMAE:  7.021416\n",
      "Epoch:  16 \tLoss:  447236320.0 \tMAE:  9.059294\n",
      "Epoch:  17 \tLoss:  417845920.0 \tMAE:  6.1877975\n",
      "Epoch:  18 \tLoss:  442040450.0 \tMAE:  7.140801\n",
      "Epoch:  19 \tLoss:  445156350.0 \tMAE:  8.374039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d_berman/Documents/Tufts_Masters/Semester_3/Bayesian Deep Learning/BDL_final_project/bnn.py:11: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  if (normalized_x.detach().numpy() >= float('inf')).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  21 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  22 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  23 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  24 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  25 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  26 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  27 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  28 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  29 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  30 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  31 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  32 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  33 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  34 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  35 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  36 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  37 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  38 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  39 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  40 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  41 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  42 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  43 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  44 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  45 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  46 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  47 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  48 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  49 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  50 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  51 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  52 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  53 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  54 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  55 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  56 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  57 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  58 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  59 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  60 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  61 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  62 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  63 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  64 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  65 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  66 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  67 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  68 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  69 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  70 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  71 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  72 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  73 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  74 \tLoss:  nan \tMAE:  nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn = BNNBayesbyBackprop(prior_mu=0.0, prior_s=1.0, num_MC_samples=30, linear_regression=True, preset=False, classification=False, input_dim=2)\n",
    "bnn.fit(X_toy, y_toy, plot=True, n_epochs=75, learning_rate=1e-3, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1_1</th>\n",
       "      <th>w1_2</th>\n",
       "      <th>w1_1_grad</th>\n",
       "      <th>w1_2_grad</th>\n",
       "      <th>b_1</th>\n",
       "      <th>b_1_grad</th>\n",
       "      <th>b_2_grad</th>\n",
       "      <th>log_prior</th>\n",
       "      <th>log_posterior</th>\n",
       "      <th>mean_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138492</td>\n",
       "      <td>-0.043693</td>\n",
       "      <td>-677.736500</td>\n",
       "      <td>-0.105935</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>415.05698</td>\n",
       "      <td>-0.173286</td>\n",
       "      <td>-8.646632</td>\n",
       "      <td>-8.491198</td>\n",
       "      <td>-2871.785677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139479</td>\n",
       "      <td>-0.042693</td>\n",
       "      <td>-537.374600</td>\n",
       "      <td>-0.107825</td>\n",
       "      <td>0.023025</td>\n",
       "      <td>409.23618</td>\n",
       "      <td>0.143933</td>\n",
       "      <td>-8.381437</td>\n",
       "      <td>-8.200098</td>\n",
       "      <td>-2886.485417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140469</td>\n",
       "      <td>-0.041887</td>\n",
       "      <td>-577.263100</td>\n",
       "      <td>-0.610362</td>\n",
       "      <td>0.022025</td>\n",
       "      <td>422.21622</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>-9.078981</td>\n",
       "      <td>-8.843579</td>\n",
       "      <td>-3126.743750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.141302</td>\n",
       "      <td>-0.041361</td>\n",
       "      <td>-41.503240</td>\n",
       "      <td>0.132136</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>418.70350</td>\n",
       "      <td>-0.095046</td>\n",
       "      <td>-8.364523</td>\n",
       "      <td>-8.167536</td>\n",
       "      <td>-3153.332292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142184</td>\n",
       "      <td>-0.040892</td>\n",
       "      <td>-597.483500</td>\n",
       "      <td>-0.029016</td>\n",
       "      <td>0.020021</td>\n",
       "      <td>439.29562</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>-7.984054</td>\n",
       "      <td>-7.868133</td>\n",
       "      <td>-2531.938281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.142961</td>\n",
       "      <td>-0.040293</td>\n",
       "      <td>-38.038180</td>\n",
       "      <td>-0.330749</td>\n",
       "      <td>0.019021</td>\n",
       "      <td>405.28262</td>\n",
       "      <td>0.034687</td>\n",
       "      <td>-8.742422</td>\n",
       "      <td>-8.476561</td>\n",
       "      <td>-2684.448438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.143790</td>\n",
       "      <td>-0.039643</td>\n",
       "      <td>-563.598200</td>\n",
       "      <td>-0.225799</td>\n",
       "      <td>0.018021</td>\n",
       "      <td>418.79898</td>\n",
       "      <td>0.257804</td>\n",
       "      <td>-9.186697</td>\n",
       "      <td>-8.891592</td>\n",
       "      <td>-3267.948698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.144550</td>\n",
       "      <td>-0.039261</td>\n",
       "      <td>-79.459470</td>\n",
       "      <td>0.257974</td>\n",
       "      <td>0.017018</td>\n",
       "      <td>440.33044</td>\n",
       "      <td>0.405441</td>\n",
       "      <td>-8.715026</td>\n",
       "      <td>-8.598409</td>\n",
       "      <td>-3034.774479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.145340</td>\n",
       "      <td>-0.038923</td>\n",
       "      <td>-419.161740</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>400.96255</td>\n",
       "      <td>-0.049325</td>\n",
       "      <td>-8.963928</td>\n",
       "      <td>-8.860149</td>\n",
       "      <td>-3651.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.146100</td>\n",
       "      <td>-0.038620</td>\n",
       "      <td>-173.408500</td>\n",
       "      <td>-0.002803</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>422.01860</td>\n",
       "      <td>0.218190</td>\n",
       "      <td>-8.680138</td>\n",
       "      <td>-8.515354</td>\n",
       "      <td>-2787.944531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.146899</td>\n",
       "      <td>-0.038209</td>\n",
       "      <td>-471.512820</td>\n",
       "      <td>-0.267667</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>430.97397</td>\n",
       "      <td>0.246297</td>\n",
       "      <td>-8.562288</td>\n",
       "      <td>-8.376757</td>\n",
       "      <td>-3113.692448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147673</td>\n",
       "      <td>-0.037820</td>\n",
       "      <td>-188.291440</td>\n",
       "      <td>-0.035909</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>405.49594</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>-8.671286</td>\n",
       "      <td>-8.488910</td>\n",
       "      <td>-3370.248437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.148351</td>\n",
       "      <td>-0.037456</td>\n",
       "      <td>60.346493</td>\n",
       "      <td>-0.020972</td>\n",
       "      <td>0.012016</td>\n",
       "      <td>435.38720</td>\n",
       "      <td>-0.095592</td>\n",
       "      <td>-9.101794</td>\n",
       "      <td>-8.886563</td>\n",
       "      <td>-3137.056771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.149079</td>\n",
       "      <td>-0.037104</td>\n",
       "      <td>-464.779750</td>\n",
       "      <td>-0.043238</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>427.48990</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>-8.429245</td>\n",
       "      <td>-8.225670</td>\n",
       "      <td>-2849.259896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.149796</td>\n",
       "      <td>-0.036897</td>\n",
       "      <td>-205.622960</td>\n",
       "      <td>0.184756</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>422.98640</td>\n",
       "      <td>0.090876</td>\n",
       "      <td>-8.385107</td>\n",
       "      <td>-8.253834</td>\n",
       "      <td>-2931.061458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.150492</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-158.103210</td>\n",
       "      <td>0.137903</td>\n",
       "      <td>0.009010</td>\n",
       "      <td>422.11765</td>\n",
       "      <td>-0.208075</td>\n",
       "      <td>-8.353212</td>\n",
       "      <td>-8.316380</td>\n",
       "      <td>-2861.411198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.151203</td>\n",
       "      <td>-0.036559</td>\n",
       "      <td>-297.449300</td>\n",
       "      <td>-0.248660</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>418.94705</td>\n",
       "      <td>0.179330</td>\n",
       "      <td>-8.731121</td>\n",
       "      <td>-8.629851</td>\n",
       "      <td>-3206.860938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.151740</td>\n",
       "      <td>-0.036186</td>\n",
       "      <td>318.569180</td>\n",
       "      <td>-0.349076</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>440.23325</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>-8.919785</td>\n",
       "      <td>-8.706644</td>\n",
       "      <td>-2871.441146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.152345</td>\n",
       "      <td>-0.035820</td>\n",
       "      <td>-453.504800</td>\n",
       "      <td>-0.051328</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>408.04422</td>\n",
       "      <td>-0.026786</td>\n",
       "      <td>-8.755681</td>\n",
       "      <td>-8.591973</td>\n",
       "      <td>-3335.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.153043</td>\n",
       "      <td>-0.035732</td>\n",
       "      <td>-676.148100</td>\n",
       "      <td>0.450986</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>428.16296</td>\n",
       "      <td>0.131417</td>\n",
       "      <td>-9.050887</td>\n",
       "      <td>-8.839693</td>\n",
       "      <td>-3408.065104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.153775</td>\n",
       "      <td>-0.035770</td>\n",
       "      <td>-411.539730</td>\n",
       "      <td>0.247044</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>425.72180</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>-8.173663</td>\n",
       "      <td>-8.098568</td>\n",
       "      <td>-3084.348698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.154422</td>\n",
       "      <td>-0.035750</td>\n",
       "      <td>70.531310</td>\n",
       "      <td>-0.112514</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>434.32022</td>\n",
       "      <td>0.112632</td>\n",
       "      <td>-8.727647</td>\n",
       "      <td>-8.543679</td>\n",
       "      <td>-3010.804167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.154959</td>\n",
       "      <td>-0.035807</td>\n",
       "      <td>175.519120</td>\n",
       "      <td>0.156855</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>426.41450</td>\n",
       "      <td>-0.025956</td>\n",
       "      <td>-8.611336</td>\n",
       "      <td>-8.480027</td>\n",
       "      <td>-3043.724740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.155479</td>\n",
       "      <td>-0.035818</td>\n",
       "      <td>-109.924904</td>\n",
       "      <td>-0.084082</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>421.90560</td>\n",
       "      <td>0.047586</td>\n",
       "      <td>-8.968108</td>\n",
       "      <td>-8.910356</td>\n",
       "      <td>-2925.847396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.155990</td>\n",
       "      <td>-0.035678</td>\n",
       "      <td>-137.238430</td>\n",
       "      <td>-0.325555</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>430.93884</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>-8.629062</td>\n",
       "      <td>-8.454385</td>\n",
       "      <td>-3513.295052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.156553</td>\n",
       "      <td>-0.035411</td>\n",
       "      <td>-379.840000</td>\n",
       "      <td>-0.320616</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>410.13663</td>\n",
       "      <td>0.153748</td>\n",
       "      <td>-9.149317</td>\n",
       "      <td>-8.944519</td>\n",
       "      <td>-3214.529167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.157100</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>-121.508780</td>\n",
       "      <td>-0.027282</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>410.09772</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>-8.535324</td>\n",
       "      <td>-8.388336</td>\n",
       "      <td>-3346.172135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.157597</td>\n",
       "      <td>-0.035021</td>\n",
       "      <td>3.145207</td>\n",
       "      <td>0.207765</td>\n",
       "      <td>-0.003022</td>\n",
       "      <td>428.28430</td>\n",
       "      <td>0.173403</td>\n",
       "      <td>-8.640804</td>\n",
       "      <td>-8.574202</td>\n",
       "      <td>-3015.413802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.158069</td>\n",
       "      <td>-0.034836</td>\n",
       "      <td>-67.494620</td>\n",
       "      <td>-0.136215</td>\n",
       "      <td>-0.004023</td>\n",
       "      <td>418.20483</td>\n",
       "      <td>0.077717</td>\n",
       "      <td>-8.798896</td>\n",
       "      <td>-8.735101</td>\n",
       "      <td>-2782.992188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.158613</td>\n",
       "      <td>-0.034521</td>\n",
       "      <td>-429.274170</td>\n",
       "      <td>-0.344850</td>\n",
       "      <td>-0.005025</td>\n",
       "      <td>435.09570</td>\n",
       "      <td>-0.090456</td>\n",
       "      <td>-8.640332</td>\n",
       "      <td>-8.498036</td>\n",
       "      <td>-3315.026302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.265449</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>-380.904080</td>\n",
       "      <td>-0.251379</td>\n",
       "      <td>-0.320797</td>\n",
       "      <td>421.09775</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>-7.156494</td>\n",
       "      <td>-8.134028</td>\n",
       "      <td>-1824.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.265407</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>-84.251144</td>\n",
       "      <td>0.281972</td>\n",
       "      <td>-0.321786</td>\n",
       "      <td>436.56516</td>\n",
       "      <td>0.065671</td>\n",
       "      <td>-7.299198</td>\n",
       "      <td>-8.524048</td>\n",
       "      <td>-2277.195052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.265342</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>83.906845</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.322772</td>\n",
       "      <td>409.21640</td>\n",
       "      <td>-0.059963</td>\n",
       "      <td>-7.382892</td>\n",
       "      <td>-8.384877</td>\n",
       "      <td>-2258.078646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.265303</td>\n",
       "      <td>0.012453</td>\n",
       "      <td>-59.907177</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>-0.323763</td>\n",
       "      <td>442.56580</td>\n",
       "      <td>-0.059607</td>\n",
       "      <td>-7.155672</td>\n",
       "      <td>-8.222015</td>\n",
       "      <td>-2259.685677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.265203</td>\n",
       "      <td>0.012305</td>\n",
       "      <td>191.690410</td>\n",
       "      <td>-0.234378</td>\n",
       "      <td>-0.324757</td>\n",
       "      <td>434.41544</td>\n",
       "      <td>-0.032751</td>\n",
       "      <td>-7.354275</td>\n",
       "      <td>-8.565254</td>\n",
       "      <td>-2059.130859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.012132</td>\n",
       "      <td>-256.663700</td>\n",
       "      <td>0.071313</td>\n",
       "      <td>-0.325750</td>\n",
       "      <td>420.11502</td>\n",
       "      <td>-0.273759</td>\n",
       "      <td>-7.197849</td>\n",
       "      <td>-8.177366</td>\n",
       "      <td>-2081.307422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.265250</td>\n",
       "      <td>0.011909</td>\n",
       "      <td>-158.524520</td>\n",
       "      <td>0.120918</td>\n",
       "      <td>-0.326744</td>\n",
       "      <td>426.42540</td>\n",
       "      <td>0.132285</td>\n",
       "      <td>-7.726491</td>\n",
       "      <td>-9.191968</td>\n",
       "      <td>-2401.279167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.265326</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>-92.029950</td>\n",
       "      <td>-0.197070</td>\n",
       "      <td>-0.327738</td>\n",
       "      <td>422.35724</td>\n",
       "      <td>-0.070405</td>\n",
       "      <td>-7.475288</td>\n",
       "      <td>-8.824050</td>\n",
       "      <td>-2407.563542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.265327</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>200.655140</td>\n",
       "      <td>0.112893</td>\n",
       "      <td>-0.328732</td>\n",
       "      <td>424.01035</td>\n",
       "      <td>-0.026947</td>\n",
       "      <td>-7.318922</td>\n",
       "      <td>-8.357669</td>\n",
       "      <td>-2274.339583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.265299</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>86.451720</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>-0.329728</td>\n",
       "      <td>429.86572</td>\n",
       "      <td>0.369896</td>\n",
       "      <td>-7.763590</td>\n",
       "      <td>-9.245480</td>\n",
       "      <td>-2336.202604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.265413</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>-412.185850</td>\n",
       "      <td>-0.030513</td>\n",
       "      <td>-0.330728</td>\n",
       "      <td>442.51000</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>-7.296003</td>\n",
       "      <td>-8.604303</td>\n",
       "      <td>-2452.936458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.265422</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>274.859900</td>\n",
       "      <td>-0.123888</td>\n",
       "      <td>-0.331723</td>\n",
       "      <td>403.42825</td>\n",
       "      <td>-0.067666</td>\n",
       "      <td>-7.393708</td>\n",
       "      <td>-8.410158</td>\n",
       "      <td>-2178.899740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.265361</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>207.080120</td>\n",
       "      <td>-0.273750</td>\n",
       "      <td>-0.332721</td>\n",
       "      <td>438.03125</td>\n",
       "      <td>-0.023315</td>\n",
       "      <td>-7.413475</td>\n",
       "      <td>-8.623665</td>\n",
       "      <td>-2207.299479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.265199</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>317.286650</td>\n",
       "      <td>0.234531</td>\n",
       "      <td>-0.333721</td>\n",
       "      <td>431.64450</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>-7.590813</td>\n",
       "      <td>-8.811782</td>\n",
       "      <td>-2341.829167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.265018</td>\n",
       "      <td>0.011542</td>\n",
       "      <td>104.563740</td>\n",
       "      <td>-0.054969</td>\n",
       "      <td>-0.334720</td>\n",
       "      <td>422.22186</td>\n",
       "      <td>-0.035938</td>\n",
       "      <td>-7.100291</td>\n",
       "      <td>-8.053075</td>\n",
       "      <td>-2242.492188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.264850</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>13.638844</td>\n",
       "      <td>-0.076295</td>\n",
       "      <td>-0.335718</td>\n",
       "      <td>419.48154</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>-7.498647</td>\n",
       "      <td>-8.624726</td>\n",
       "      <td>-2416.196354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.264726</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>-78.675354</td>\n",
       "      <td>0.111245</td>\n",
       "      <td>-0.336714</td>\n",
       "      <td>417.73157</td>\n",
       "      <td>0.112304</td>\n",
       "      <td>-7.362716</td>\n",
       "      <td>-8.502427</td>\n",
       "      <td>-2353.947917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.264491</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>363.498700</td>\n",
       "      <td>0.210450</td>\n",
       "      <td>-0.337713</td>\n",
       "      <td>439.99550</td>\n",
       "      <td>0.136689</td>\n",
       "      <td>-7.134041</td>\n",
       "      <td>-8.095454</td>\n",
       "      <td>-2054.068359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.264156</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>366.850620</td>\n",
       "      <td>-0.079363</td>\n",
       "      <td>-0.338720</td>\n",
       "      <td>456.71634</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>-7.246292</td>\n",
       "      <td>-8.286881</td>\n",
       "      <td>-2313.537240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.263952</td>\n",
       "      <td>0.011327</td>\n",
       "      <td>-289.882750</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>-0.339717</td>\n",
       "      <td>387.09122</td>\n",
       "      <td>0.126824</td>\n",
       "      <td>-7.287172</td>\n",
       "      <td>-8.289127</td>\n",
       "      <td>-1801.400521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.263759</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>28.426110</td>\n",
       "      <td>0.088218</td>\n",
       "      <td>-0.340716</td>\n",
       "      <td>429.41107</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>-7.415360</td>\n",
       "      <td>-8.703872</td>\n",
       "      <td>-2209.405729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.263447</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>407.059400</td>\n",
       "      <td>-0.018007</td>\n",
       "      <td>-0.341709</td>\n",
       "      <td>400.82886</td>\n",
       "      <td>0.090745</td>\n",
       "      <td>-7.443266</td>\n",
       "      <td>-8.465537</td>\n",
       "      <td>-2172.242448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.263058</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>323.852570</td>\n",
       "      <td>0.035129</td>\n",
       "      <td>-0.342704</td>\n",
       "      <td>430.84576</td>\n",
       "      <td>0.152271</td>\n",
       "      <td>-7.083241</td>\n",
       "      <td>-8.051358</td>\n",
       "      <td>-2088.025130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.262744</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>-108.757610</td>\n",
       "      <td>0.163922</td>\n",
       "      <td>-0.343701</td>\n",
       "      <td>435.24777</td>\n",
       "      <td>0.205060</td>\n",
       "      <td>-7.194461</td>\n",
       "      <td>-8.301904</td>\n",
       "      <td>-1936.174870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.262505</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>-131.059560</td>\n",
       "      <td>-0.130314</td>\n",
       "      <td>-0.344700</td>\n",
       "      <td>430.35886</td>\n",
       "      <td>0.136739</td>\n",
       "      <td>-7.476802</td>\n",
       "      <td>-8.871672</td>\n",
       "      <td>-2566.764583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.262323</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>-97.356630</td>\n",
       "      <td>-0.107833</td>\n",
       "      <td>-0.345698</td>\n",
       "      <td>423.95227</td>\n",
       "      <td>-0.132540</td>\n",
       "      <td>-7.509211</td>\n",
       "      <td>-8.727040</td>\n",
       "      <td>-2097.272786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.262236</td>\n",
       "      <td>0.010591</td>\n",
       "      <td>-226.105930</td>\n",
       "      <td>0.196822</td>\n",
       "      <td>-0.346695</td>\n",
       "      <td>415.63113</td>\n",
       "      <td>-0.213411</td>\n",
       "      <td>-7.234032</td>\n",
       "      <td>-8.472186</td>\n",
       "      <td>-2053.783724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.262274</td>\n",
       "      <td>0.010352</td>\n",
       "      <td>-343.779800</td>\n",
       "      <td>0.211369</td>\n",
       "      <td>-0.347691</td>\n",
       "      <td>423.06067</td>\n",
       "      <td>0.051922</td>\n",
       "      <td>-7.159554</td>\n",
       "      <td>-8.186648</td>\n",
       "      <td>-1824.487891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.262311</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>-7.276535</td>\n",
       "      <td>-0.055578</td>\n",
       "      <td>-0.348689</td>\n",
       "      <td>433.72903</td>\n",
       "      <td>0.105894</td>\n",
       "      <td>-7.333482</td>\n",
       "      <td>-8.661740</td>\n",
       "      <td>-2728.325260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.262380</td>\n",
       "      <td>0.010062</td>\n",
       "      <td>-105.899860</td>\n",
       "      <td>-0.100383</td>\n",
       "      <td>-0.349688</td>\n",
       "      <td>427.13388</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>-7.107447</td>\n",
       "      <td>-8.173778</td>\n",
       "      <td>-1981.037630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1_1      w1_2   w1_1_grad  w1_2_grad       b_1   b_1_grad  b_2_grad  \\\n",
       "0    0.138492 -0.043693 -677.736500  -0.105935  0.024025  415.05698 -0.173286   \n",
       "1    0.139479 -0.042693 -537.374600  -0.107825  0.023025  409.23618  0.143933   \n",
       "2    0.140469 -0.041887 -577.263100  -0.610362  0.022025  422.21622  0.032742   \n",
       "3    0.141302 -0.041361  -41.503240   0.132136  0.021024  418.70350 -0.095046   \n",
       "4    0.142184 -0.040892 -597.483500  -0.029016  0.020021  439.29562  0.231250   \n",
       "5    0.142961 -0.040293  -38.038180  -0.330749  0.019021  405.28262  0.034687   \n",
       "6    0.143790 -0.039643 -563.598200  -0.225799  0.018021  418.79898  0.257804   \n",
       "7    0.144550 -0.039261  -79.459470   0.257974  0.017018  440.33044  0.405441   \n",
       "8    0.145340 -0.038923 -419.161740   0.002739  0.016019  400.96255 -0.049325   \n",
       "9    0.146100 -0.038620 -173.408500  -0.002803  0.015019  422.01860  0.218190   \n",
       "10   0.146899 -0.038209 -471.512820  -0.267667  0.014017  430.97397  0.246297   \n",
       "11   0.147673 -0.037820 -188.291440  -0.035909  0.013018  405.49594  0.040246   \n",
       "12   0.148351 -0.037456   60.346493  -0.020972  0.012016  435.38720 -0.095592   \n",
       "13   0.149079 -0.037104 -464.779750  -0.043238  0.011014  427.48990  0.042343   \n",
       "14   0.149796 -0.036897 -205.622960   0.184756  0.010012  422.98640  0.090876   \n",
       "15   0.150492 -0.036788 -158.103210   0.137903  0.009010  422.11765 -0.208075   \n",
       "16   0.151203 -0.036559 -297.449300  -0.248660  0.008008  418.94705  0.179330   \n",
       "17   0.151740 -0.036186  318.569180  -0.349076  0.007004  440.23325  0.422743   \n",
       "18   0.152345 -0.035820 -453.504800  -0.051328  0.006002  408.04422 -0.026786   \n",
       "19   0.153043 -0.035732 -676.148100   0.450986  0.005000  428.16296  0.131417   \n",
       "20   0.153775 -0.035770 -411.539730   0.247044  0.003997  425.72180  0.160311   \n",
       "21   0.154422 -0.035750   70.531310  -0.112514  0.002993  434.32022  0.112632   \n",
       "22   0.154959 -0.035807  175.519120   0.156855  0.001989  426.41450 -0.025956   \n",
       "23   0.155479 -0.035818 -109.924904  -0.084082  0.000985  421.90560  0.047586   \n",
       "24   0.155990 -0.035678 -137.238430  -0.325555 -0.000020  430.93884  0.182695   \n",
       "25   0.156553 -0.035411 -379.840000  -0.320616 -0.001022  410.13663  0.153748   \n",
       "26   0.157100 -0.035156 -121.508780  -0.027282 -0.002022  410.09772  0.044447   \n",
       "27   0.157597 -0.035021    3.145207   0.207765 -0.003022  428.28430  0.173403   \n",
       "28   0.158069 -0.034836  -67.494620  -0.136215 -0.004023  418.20483  0.077717   \n",
       "29   0.158613 -0.034521 -429.274170  -0.344850 -0.005025  435.09570 -0.090456   \n",
       "..        ...       ...         ...        ...       ...        ...       ...   \n",
       "345  0.265449  0.013203 -380.904080  -0.251379 -0.320797  421.09775  0.041322   \n",
       "346  0.265407  0.012975  -84.251144   0.281972 -0.321786  436.56516  0.065671   \n",
       "347  0.265342  0.012763   83.906845   0.013981 -0.322772  409.21640 -0.059963   \n",
       "348  0.265303  0.012453  -59.907177   0.211765 -0.323763  442.56580 -0.059607   \n",
       "349  0.265203  0.012305  191.690410  -0.234378 -0.324757  434.41544 -0.032751   \n",
       "350  0.265200  0.012132 -256.663700   0.071313 -0.325750  420.11502 -0.273759   \n",
       "351  0.265250  0.011909 -158.524520   0.120918 -0.326744  426.42540  0.132285   \n",
       "352  0.265326  0.011818  -92.029950  -0.197070 -0.327738  422.35724 -0.070405   \n",
       "353  0.265327  0.011673  200.655140   0.112893 -0.328732  424.01035 -0.026947   \n",
       "354  0.265299  0.011537   86.451720   0.009576 -0.329728  429.86572  0.369896   \n",
       "355  0.265413  0.011432 -412.185850  -0.030513 -0.330728  442.51000  0.030847   \n",
       "356  0.265422  0.011406  274.859900  -0.123888 -0.331723  403.42825 -0.067666   \n",
       "357  0.265361  0.011537  207.080120  -0.273750 -0.332721  438.03125 -0.023315   \n",
       "358  0.265199  0.011523  317.286650   0.234531 -0.333721  431.64450  0.134116   \n",
       "359  0.265018  0.011542  104.563740  -0.054969 -0.334720  422.22186 -0.035938   \n",
       "360  0.264850  0.011601   13.638844  -0.076295 -0.335718  419.48154  0.231190   \n",
       "361  0.264726  0.011592  -78.675354   0.111245 -0.336714  417.73157  0.112304   \n",
       "362  0.264491  0.011466  363.498700   0.210450 -0.337713  439.99550  0.136689   \n",
       "363  0.264156  0.011396  366.850620  -0.079363 -0.338720  456.71634  0.016647   \n",
       "364  0.263952  0.011327 -289.882750   0.012488 -0.339717  387.09122  0.126824   \n",
       "365  0.263759  0.011215   28.426110   0.088218 -0.340716  429.41107  0.030603   \n",
       "366  0.263447  0.011123  407.059400  -0.018007 -0.341709  400.82886  0.090745   \n",
       "367  0.263058  0.011021  323.852570   0.035129 -0.342704  430.84576  0.152271   \n",
       "368  0.262744  0.010837 -108.757610   0.163922 -0.343701  435.24777  0.205060   \n",
       "369  0.262505  0.010744 -131.059560  -0.130314 -0.344700  430.35886  0.136739   \n",
       "370  0.262323  0.010722  -97.356630  -0.107833 -0.345698  423.95227 -0.132540   \n",
       "371  0.262236  0.010591 -226.105930   0.196822 -0.346695  415.63113 -0.213411   \n",
       "372  0.262274  0.010352 -343.779800   0.211369 -0.347691  423.06067  0.051922   \n",
       "373  0.262311  0.010169   -7.276535  -0.055578 -0.348689  433.72903  0.105894   \n",
       "374  0.262380  0.010062 -105.899860  -0.100383 -0.349688  427.13388  0.019335   \n",
       "\n",
       "     log_prior  log_posterior  mean_likelihood  \n",
       "0    -8.646632      -8.491198     -2871.785677  \n",
       "1    -8.381437      -8.200098     -2886.485417  \n",
       "2    -9.078981      -8.843579     -3126.743750  \n",
       "3    -8.364523      -8.167536     -3153.332292  \n",
       "4    -7.984054      -7.868133     -2531.938281  \n",
       "5    -8.742422      -8.476561     -2684.448438  \n",
       "6    -9.186697      -8.891592     -3267.948698  \n",
       "7    -8.715026      -8.598409     -3034.774479  \n",
       "8    -8.963928      -8.860149     -3651.906250  \n",
       "9    -8.680138      -8.515354     -2787.944531  \n",
       "10   -8.562288      -8.376757     -3113.692448  \n",
       "11   -8.671286      -8.488910     -3370.248437  \n",
       "12   -9.101794      -8.886563     -3137.056771  \n",
       "13   -8.429245      -8.225670     -2849.259896  \n",
       "14   -8.385107      -8.253834     -2931.061458  \n",
       "15   -8.353212      -8.316380     -2861.411198  \n",
       "16   -8.731121      -8.629851     -3206.860938  \n",
       "17   -8.919785      -8.706644     -2871.441146  \n",
       "18   -8.755681      -8.591973     -3335.010417  \n",
       "19   -9.050887      -8.839693     -3408.065104  \n",
       "20   -8.173663      -8.098568     -3084.348698  \n",
       "21   -8.727647      -8.543679     -3010.804167  \n",
       "22   -8.611336      -8.480027     -3043.724740  \n",
       "23   -8.968108      -8.910356     -2925.847396  \n",
       "24   -8.629062      -8.454385     -3513.295052  \n",
       "25   -9.149317      -8.944519     -3214.529167  \n",
       "26   -8.535324      -8.388336     -3346.172135  \n",
       "27   -8.640804      -8.574202     -3015.413802  \n",
       "28   -8.798896      -8.735101     -2782.992188  \n",
       "29   -8.640332      -8.498036     -3315.026302  \n",
       "..         ...            ...              ...  \n",
       "345  -7.156494      -8.134028     -1824.416667  \n",
       "346  -7.299198      -8.524048     -2277.195052  \n",
       "347  -7.382892      -8.384877     -2258.078646  \n",
       "348  -7.155672      -8.222015     -2259.685677  \n",
       "349  -7.354275      -8.565254     -2059.130859  \n",
       "350  -7.197849      -8.177366     -2081.307422  \n",
       "351  -7.726491      -9.191968     -2401.279167  \n",
       "352  -7.475288      -8.824050     -2407.563542  \n",
       "353  -7.318922      -8.357669     -2274.339583  \n",
       "354  -7.763590      -9.245480     -2336.202604  \n",
       "355  -7.296003      -8.604303     -2452.936458  \n",
       "356  -7.393708      -8.410158     -2178.899740  \n",
       "357  -7.413475      -8.623665     -2207.299479  \n",
       "358  -7.590813      -8.811782     -2341.829167  \n",
       "359  -7.100291      -8.053075     -2242.492188  \n",
       "360  -7.498647      -8.624726     -2416.196354  \n",
       "361  -7.362716      -8.502427     -2353.947917  \n",
       "362  -7.134041      -8.095454     -2054.068359  \n",
       "363  -7.246292      -8.286881     -2313.537240  \n",
       "364  -7.287172      -8.289127     -1801.400521  \n",
       "365  -7.415360      -8.703872     -2209.405729  \n",
       "366  -7.443266      -8.465537     -2172.242448  \n",
       "367  -7.083241      -8.051358     -2088.025130  \n",
       "368  -7.194461      -8.301904     -1936.174870  \n",
       "369  -7.476802      -8.871672     -2566.764583  \n",
       "370  -7.509211      -8.727040     -2097.272786  \n",
       "371  -7.234032      -8.472186     -2053.783724  \n",
       "372  -7.159554      -8.186648     -1824.487891  \n",
       "373  -7.333482      -8.661740     -2728.325260  \n",
       "374  -7.107447      -8.173778     -1981.037630  \n",
       "\n",
       "[375 rows x 10 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"logging.csv\")\n",
    "df = df.drop(['w2_1', 'w2_2', 'b_2', 'w2_1_grad', 'w2_2_grad'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 5000\n",
    "\n",
    "X_train = np.empty(shape=(dataset_size, 2), dtype=float)\n",
    "y_train = np.empty(shape=(dataset_size,), dtype=float)\n",
    "\n",
    "# make first dimension 0 to effectively make it univariate \n",
    "# bc current bnn arch only works with multivariate input\n",
    "X_train[:int(dataset_size/2),0] = np.random.normal(loc=0,\n",
    "                                                   scale=1,\n",
    "                                                   size=(int(dataset_size/2),))\n",
    "X_train[:int(dataset_size/2), 1] = np.random.normal(loc=-5, \n",
    "                                                    scale=1, \n",
    "                                                    size=(int(dataset_size/2),))\n",
    "y_train[:int(dataset_size/2)] = 0\n",
    "\n",
    "X_train[int(dataset_size/2):,0] = np.random.normal(loc=0,\n",
    "                                                   scale=1,\n",
    "                                                   size=(int(dataset_size/2),))\n",
    "X_train[int(dataset_size/2):, 1] = np.random.normal(loc=5,\n",
    "                                                    scale=1,\n",
    "                                                    size=(int(dataset_size/2),))\n",
    "y_train[int(dataset_size/2):] = 1\n",
    "\n",
    "# shuffle order\n",
    "perm = np.random.permutation(dataset_size)\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data being saved in following file:\n",
      "logging.csv\n",
      "Epoch:  0 \tLoss:  6693.56 \tacc:  0.966\n",
      "Epoch:  1 \tLoss:  4897.3887 \tacc:  1.0\n",
      "Epoch:  2 \tLoss:  4959.336 \tacc:  0.0\n",
      "Epoch:  3 \tLoss:  3822.735 \tacc:  0.9252\n",
      "Epoch:  4 \tLoss:  3060.7334 \tacc:  0.9376\n",
      "Epoch:  5 \tLoss:  3857.6797 \tacc:  0.9996\n",
      "Epoch:  6 \tLoss:  2974.9458 \tacc:  0.998\n",
      "Epoch:  7 \tLoss:  2414.552 \tacc:  1.0\n",
      "Epoch:  8 \tLoss:  1891.903 \tacc:  1.0\n",
      "Epoch:  9 \tLoss:  1303.2712 \tacc:  0.9656\n",
      "Epoch:  10 \tLoss:  1606.8391 \tacc:  0.7826\n",
      "Epoch:  11 \tLoss:  1287.3357 \tacc:  0.0708\n",
      "Epoch:  12 \tLoss:  1143.8702 \tacc:  1.0\n",
      "Epoch:  13 \tLoss:  1087.5385 \tacc:  0.7472\n",
      "Epoch:  14 \tLoss:  899.7684 \tacc:  0.9998\n",
      "Epoch:  15 \tLoss:  1072.9089 \tacc:  1.0\n",
      "Epoch:  16 \tLoss:  983.24164 \tacc:  1.0\n",
      "Epoch:  17 \tLoss:  786.44275 \tacc:  1.0\n",
      "Epoch:  18 \tLoss:  665.1627 \tacc:  1.0\n",
      "Epoch:  19 \tLoss:  645.5856 \tacc:  1.0\n",
      "Epoch:  20 \tLoss:  534.66876 \tacc:  1.0\n",
      "Epoch:  21 \tLoss:  640.41296 \tacc:  1.0\n",
      "Epoch:  22 \tLoss:  471.17853 \tacc:  1.0\n",
      "Epoch:  23 \tLoss:  332.2183 \tacc:  1.0\n",
      "Epoch:  24 \tLoss:  389.7181 \tacc:  0.9944\n",
      "Epoch:  25 \tLoss:  264.10794 \tacc:  1.0\n",
      "Epoch:  26 \tLoss:  274.65564 \tacc:  1.0\n",
      "Epoch:  27 \tLoss:  277.1217 \tacc:  1.0\n",
      "Epoch:  28 \tLoss:  210.96936 \tacc:  1.0\n",
      "Epoch:  29 \tLoss:  225.15028 \tacc:  1.0\n",
      "Epoch:  30 \tLoss:  182.7843 \tacc:  1.0\n",
      "Epoch:  31 \tLoss:  212.97153 \tacc:  1.0\n",
      "Epoch:  32 \tLoss:  188.26044 \tacc:  1.0\n",
      "Epoch:  33 \tLoss:  223.9276 \tacc:  1.0\n",
      "Epoch:  34 \tLoss:  138.8989 \tacc:  1.0\n",
      "Epoch:  35 \tLoss:  154.58475 \tacc:  1.0\n",
      "Epoch:  36 \tLoss:  127.88 \tacc:  1.0\n",
      "Epoch:  37 \tLoss:  117.62117 \tacc:  0.998\n",
      "Epoch:  38 \tLoss:  125.759796 \tacc:  1.0\n",
      "Epoch:  39 \tLoss:  112.6349 \tacc:  0.9986\n",
      "Epoch:  40 \tLoss:  121.05345 \tacc:  1.0\n",
      "Epoch:  41 \tLoss:  146.69121 \tacc:  0.9134\n",
      "Epoch:  42 \tLoss:  138.89882 \tacc:  1.0\n",
      "Epoch:  43 \tLoss:  120.35684 \tacc:  1.0\n",
      "Epoch:  44 \tLoss:  119.08445 \tacc:  0.9996\n",
      "Epoch:  45 \tLoss:  76.727554 \tacc:  1.0\n",
      "Epoch:  46 \tLoss:  95.31822 \tacc:  1.0\n",
      "Epoch:  47 \tLoss:  95.131714 \tacc:  1.0\n",
      "Epoch:  48 \tLoss:  83.47969 \tacc:  1.0\n",
      "Epoch:  49 \tLoss:  74.89314 \tacc:  1.0\n",
      "Epoch:  50 \tLoss:  85.77935 \tacc:  1.0\n",
      "Epoch:  51 \tLoss:  105.588524 \tacc:  1.0\n",
      "Epoch:  52 \tLoss:  81.32758 \tacc:  1.0\n",
      "Epoch:  53 \tLoss:  67.72488 \tacc:  1.0\n",
      "Epoch:  54 \tLoss:  69.0169 \tacc:  1.0\n",
      "Epoch:  55 \tLoss:  71.12326 \tacc:  1.0\n",
      "Epoch:  56 \tLoss:  97.4014 \tacc:  1.0\n",
      "Epoch:  57 \tLoss:  90.87042 \tacc:  1.0\n",
      "Epoch:  58 \tLoss:  52.4627 \tacc:  1.0\n",
      "Epoch:  59 \tLoss:  54.740265 \tacc:  1.0\n",
      "Epoch:  60 \tLoss:  66.32948 \tacc:  0.999\n",
      "Epoch:  61 \tLoss:  66.54288 \tacc:  1.0\n",
      "Epoch:  62 \tLoss:  59.604065 \tacc:  1.0\n",
      "Epoch:  63 \tLoss:  52.91794 \tacc:  1.0\n",
      "Epoch:  64 \tLoss:  65.18332 \tacc:  1.0\n",
      "Epoch:  65 \tLoss:  53.62926 \tacc:  1.0\n",
      "Epoch:  66 \tLoss:  43.103508 \tacc:  1.0\n",
      "Epoch:  67 \tLoss:  45.67466 \tacc:  1.0\n",
      "Epoch:  68 \tLoss:  48.52401 \tacc:  1.0\n",
      "Epoch:  69 \tLoss:  59.941593 \tacc:  1.0\n",
      "Epoch:  70 \tLoss:  37.222977 \tacc:  1.0\n",
      "Epoch:  71 \tLoss:  33.893314 \tacc:  1.0\n",
      "Epoch:  72 \tLoss:  41.551136 \tacc:  1.0\n",
      "Epoch:  73 \tLoss:  38.08517 \tacc:  1.0\n",
      "Epoch:  74 \tLoss:  48.344128 \tacc:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt83HWd7/HXZyaZJDNJmmtvSa+00Fa51QiVIiugUFi1PEQ86O5akWM9Z1kvq2cV3HOWXT2u7tn1jstZVlDw7Kos6wqLrFgRZUGBFrm3lKb30LRJm6RJkyaTZD7nj/mlTJpJk7ZJZjLzfj4e85iZ7+87k8807bz7+35/v+/P3B0REck/oUwXICIimaEAEBHJUwoAEZE8pQAQEclTCgARkTylABARyVNjBoCZnWVmz6XcOs3sk2ZWZWYbzGxbcF8Z9Dcz+6aZNZrZC2a2MuW91gX9t5nZusn8YCIicmJ2MucBmFkYeA24ELgJaHP3L5vZzUClu3/WzK4GPgZcHfT7hrtfaGZVwCagAXDgGeBN7t4+oZ9IRETG5WSHgC4Htrv7bmAtcHfQfjdwTfB4LXCPJz0JVJjZHOBKYIO7twVf+huANaf9CURE5JQUnGT/64EfBI9nuXszgLs3m9nMoL0O2JvymqagbbT2UdXU1PjChQtPskQRkfz2zDPPHHT32rH6jTsAzCwCvBu4Zayuadr8BO3H/5z1wHqA+fPns2nTpvGWKCIigJntHk+/kxkCugr4nbsfCJ4fCIZ2CO5bgvYmYF7K6+qBfSdoH8bd73D3BndvqK0dM8BEROQUnUwAvJ/Xh38AHgCGjuRZB9yf0v7B4GigVcDhYKjoYeAKM6sMjhi6ImgTEZEMGNcQkJlFgXcAH01p/jJwr5ndCOwBrgvaHyJ5BFAj0APcAODubWb2BWBj0O/z7t522p9AREROyUkdBjrVGhoaXHMAIiInx8yecfeGsfrpTGARkTylABARyVMKABGRPJWTAbCv4yhf/flWdh7sznQpIiJZKycDoK07zjd/2cjW/V2ZLkVEJGvlZABUxSIAdPTEM1yJiEj2yskAqIwmA6BNASAiMqqcDICSSJjiwhDt3QoAEZHR5GQAAFRFI7R192e6DBGRrJWzAVAZi2gOQETkBHI2AKpiEc0BiIicQM4GQEU0ojkAEZETyNkAqIoW0t6jOQARkdHkbABUxiIcPtrPwGAi06WIiGSlnA2AYyeDHdVegIhIOjkbABXByWCaBxARSS9nA6BqKAA0DyAiklbOBkBlrBBILgwnIiIj5WwADM0BtOtcABGRtHI2AI4tCKc9ABGRtHI2AIoLw5QUhrUchIjIKMYVAGZWYWb3mdkrZrbFzN5iZlVmtsHMtgX3lUFfM7Nvmlmjmb1gZitT3mdd0H+bma2brA81pCqmBeFEREYz3j2AbwA/c/dlwLnAFuBm4BF3Xwo8EjwHuApYGtzWA7cDmFkVcCtwIXABcOtQaEyWylih5gBEREYxZgCYWTlwCXAngLvH3b0DWAvcHXS7G7gmeLwWuMeTngQqzGwOcCWwwd3b3L0d2ACsmdBPc5zKaERzACIioxjPHsBioBX4rpk9a2bfMbMYMMvdmwGC+5lB/zpgb8rrm4K20donTWVUS0KLiIxmPAFQAKwEbnf384FuXh/uScfStPkJ2oe/2Gy9mW0ys02tra3jKG90yTkABYCISDrjCYAmoMndnwqe30cyEA4EQzsE9y0p/eelvL4e2HeC9mHc/Q53b3D3htra2pP5LCNURiN09g7QrwXhRERGGDMA3H0/sNfMzgqaLgc2Aw8AQ0fyrAPuDx4/AHwwOBpoFXA4GCJ6GLjCzCqDyd8rgrZJM3Q2cIeWgxARGaFgnP0+BvyTmUWAHcANJMPjXjO7EdgDXBf0fQi4GmgEeoK+uHubmX0B2Bj0+7y7t03IpxjF0MlgHT1xasuKJvNHiYhMO+MKAHd/DmhIs+nyNH0duGmU97kLuOtkCjwdQ8tBaB5ARGSknD0TGF7fA9C5ACIiI+V2AARzAFoSWkRkpNwOAC0IJyIyqpwOgOLCMNFIWFcFExFJI6cDAILlIDQHICIyQu4HQKxQ5wGIiKSR+wGgBeFERNLK+QCoikV0GKiISBo5HwDaAxARSS8vAqBLC8KJiIyQ8wFQpQXhRETSyvkAqIxpOQgRkXRyPgCqdDawiEhaOR8AFSlLQouIyOtyPgBeXxJacwAiIqlyPgAqokMrgmoPQEQkVc4HQHFhmFgkrDkAEZHj5HwAQPJIIO0BiIgMlx8BEI1oSWgRkePkRwDEIrTpRDARkWHyIgCqooU6DFRE5DjjCgAz22VmL5rZc2a2KWirMrMNZrYtuK8M2s3MvmlmjWb2gpmtTHmfdUH/bWa2bnI+0kiVMS0IJyJyvJPZA7jU3c9z94bg+c3AI+6+FHgkeA5wFbA0uK0HbodkYAC3AhcCFwC3DoXGZNOCcCIiI53OENBa4O7g8d3ANSnt93jSk0CFmc0BrgQ2uHubu7cDG4A1p/Hzx03rAYmIjDTeAHDg52b2jJmtD9pmuXszQHA/M2ivA/amvLYpaButfdJVHVsOQhPBIiJDCsbZb7W77zOzmcAGM3vlBH0tTZufoH34i5MBsx5g/vz54yzvxCqDJaHbuuM0Hz7K936zi/s2NfGZNWfxX948MT9DRGS6GVcAuPu+4L7FzP6N5Bj+ATOb4+7NwRBPS9C9CZiX8vJ6YF/Q/rbj2n+V5mfdAdwB0NDQMCIgTkVlsAfwv3+6mVeau0i4Ew4ZG3e1KwBEJG+NOQRkZjEzKxt6DFwBvAQ8AAwdybMOuD94/ADwweBooFXA4WCI6GHgCjOrDCZ/rwjaJt2s8mLCIWPXwR4+dNFCfv1nl3J23Qxeaz86FT9eRCQrjWcPYBbwb2Y21P+f3f1nZrYRuNfMbgT2ANcF/R8CrgYagR7gBgB3bzOzLwAbg36fd/e2CfskJ1AVi/DTj1/M3IoSyouTw0H1lVGe29sxFT9eRCQrjRkA7r4DODdN+yHg8jTtDtw0ynvdBdx18mWevmWzy4c9r6ss4aEXmxlMJIeDRETyTV6cCZxOXUUJAwmnpas306WIiGRE/gZAZQmA5gFEJG/lbQDUVwQB0KEAEJH8lLcBMLQH0KQ9ABHJU3kbANFIAVWxiAJARPJW3gYAJCeCNQQkIvlKAdDek+kyREQyIr8DoDK5B5A8dUFEJL/kdQDUV5bQ25/gkC4WIyJ5KK8DoK5C5wKISP7K7wCo1LkAIpK/8joA6iuigPYARCQ/5XUAlJcUUFpUoD0AEclLeR0AZkZ9ZQlNOhRURPJQXgcAJCeCdTawiOQjBUClzgYWkfykAKgooat3gM7e/kyXIiIypfI+AOordSSQiOSnvA8ALQstIvlKAXDsbGAdCSQi+SXvA6CmNEJRQUgTwSKSd8YdAGYWNrNnzezB4PkiM3vKzLaZ2Y/MLBK0FwXPG4PtC1Pe45agfauZXTnRH+ZUmJmuCyAieelk9gA+AWxJef43wNfcfSnQDtwYtN8ItLv7EuBrQT/MbAVwPfAGYA3w92YWPr3yJ0ZdZYkmgUUk74wrAMysHvh94DvBcwMuA+4LutwNXBM8Xhs8J9h+edB/LfBDd+9z951AI3DBRHyI05U8G1gBICL5Zbx7AF8HPgMkgufVQIe7DwTPm4C64HEdsBcg2H446H+sPc1rjjGz9Wa2ycw2tba2nsRHOXV1FSUc6o5zND44JT9PRCQbjBkAZvZOoMXdn0ltTtPVx9h2ote83uB+h7s3uHtDbW3tWOVNCC0LLSL5aDx7AKuBd5vZLuCHJId+vg5UmFlB0Kce2Bc8bgLmAQTbZwBtqe1pXpNRdUPLQisARCSPjBkA7n6Lu9e7+0KSk7i/dPc/AB4F3ht0WwfcHzx+IHhOsP2Xnrzo7gPA9cFRQouApcDTE/ZJTkN95dhXBjt0pI/dh7qnqiQRkUl3OucBfBb4lJk1khzjvzNovxOoDto/BdwM4O4vA/cCm4GfATe5e1YMus8qL6YgZCdcFvrzD25m3V1ZkVciIhOiYOwur3P3XwG/Ch7vIM1RPO7eC1w3yuu/CHzxZIucbOGQMbeihD1towfAi68dZtehHnriA0QjJ/XHJiKSlfL+TOAhZ84q45X9XWm39fYPsutgcvhnR6uGgUQkNygAAsvnlLGj9Qi9/SNHpRpbjpAIjlfa3npkiisTEZkcCoDA8jnlJBy2HRj5Bf/qgdf3DLZrD0BEcoQCILBsdhkAW/Z3jti2dX8XkXCI+soStrdoD0BEcoNmMwMLqmOUFIbZ0pwmAA50ccbMUubOKNYQkIjkDO0BBMIh48zZZbzSPHIieOv+LpbNLuOMmaXsONjNYGLECcwiItOOAiDFijllbNnfSfK8taTDR/tpPtzLmbPKOKM2RnwgoZVDRSQnKABSLJ9TTkdPP/s7e4+1DU0AL5tdxpKZpYCOBBKR3KAASLFsdjnAsGGgrcG5AWfOLmNxTTIAGjURLCI5QAGQYtmc5JFAm1Mmgrfu76KsqIC5M4qpjEWojkW0ByAiOUEBkKK8uJC6ipJhZwRvPdDFmbPLSF7TBs6oLVUAiEhOUAAcZ/mc8mOHgro7W/d3cVZwjgDAGTNjOhlMRHKCAuA4qUtCtHT1cfhoP2fNSgmA2lLauuO0dcczWKWIyOlTABwndUmIoaGg4XsAOhJIRHKDAuA4x5aEaO7k1aEASNkDWFIbBICOBBKRaU5LQRzn2JIQ+zvpPDrAzLIiKmORY9vnVpRQVBDSHoCITHsKgOOEQ8ZZs8vY0txJd9/gsOGfoe2LajQRLCLTn4aA0lg+p4wtzV1sa+kaNvwz5IyZOhRURKY/BUAay+eUc/hoP739Cc6cPTIAltSWsretJ+3FY0REpgsFQBpDS0IkH6ffA0g47DqkYSARmb7GDAAzKzazp83seTN72cz+KmhfZGZPmdk2M/uRmUWC9qLgeWOwfWHKe90StG81sysn60OdrqElIcxg6cw0AVAbA2B7iwJARKav8ewB9AGXufu5wHnAGjNbBfwN8DV3Xwq0AzcG/W8E2t19CfC1oB9mtgK4HngDsAb4ezMLT+SHmShDS0IsqIpSEhlZ4tCicJoHEJHpbMwA8KShb7rC4ObAZcB9QfvdwDXB47XBc4Ltl1tyIZ21wA/dvc/ddwKNwAUT8ikmwYcuWsgfrlqQdltJJExdRYkCQESmtXEdBhr8T/0ZYAnwbWA70OHuA0GXJqAueFwH7AVw9wEzOwxUB+1Pprxt6muyzkcuWXzC7WfMLNWy0CIyrY1rEtjdB939PKCe5P/al6frFtzbKNtGax/GzNab2SYz29Ta2jqe8jJiSbAq6MBgItOliIickpM6CsjdO4BfAauACjMb2oOoB/YFj5uAeQDB9hlAW2p7mtek/ow73L3B3Rtqa2tPprwpdf78Cnr7E7y0b+RF5EVEpoPxHAVUa2YVweMS4O3AFuBR4L1Bt3XA/cHjB4LnBNt/6cmL7D4AXB8cJbQIWAo8PVEfZKpduLgKgKd2HMpwJSIip2Y8ewBzgEfN7AVgI7DB3R8EPgt8yswaSY7x3xn0vxOoDto/BdwM4O4vA/cCm4GfATe5+7Q9k2pmWTGLa2M8qQAQkWlqzElgd38BOD9N+w7SHMXj7r3AdaO81xeBL558mdnpwkXVPPj8PgYTTjiUbopDRCR76Uzg07BqcRVdfQNs1jyAiExDCoDTsGpxNYCGgURkWlIAnIZZ5cUsrI7y1E4FgIhMPwqA07RqcTVP72xjMDHilAYRkaymADhNFy6uorN3gC3NmgcQkelFAXCaLlyUnAd4amdbhisRETk5CoDTNLeihPlVUZ0QJiLTjgJgAly4qIqnd7WR0DyAiEwjCoAJsGpxNR09/Ww90JXpUkRExk0BMAG0LpCITEcKgAlQXxmlrqKEJ3doIlhEpg8FwARZtbiap3e1kVz4VEQk+ykAJsiqxVW0dcd5Zb/mAURkelAATJCLltQA8ETjwQxXIiIyPgqACVJXUcLimpgCQESmDQXABFq9pIandrYRH9B1gkUk+ykAJtDqJTX0xAd5vqkj06WIiIxJATCB3rK4mpDB49s0DCQi2U8BMIFmRAs5u26G5gFEZFpQAEyw1UtqeHZvB129/ZkuRUTkhBQAE+ziJTUMJpyntTy0iGS5MQPAzOaZ2aNmtsXMXjazTwTtVWa2wcy2BfeVQbuZ2TfNrNHMXjCzlSnvtS7ov83M1k3ex8qclQsqKSoI8biGgUQky41nD2AA+LS7LwdWATeZ2QrgZuARd18KPBI8B7gKWBrc1gO3QzIwgFuBC4ELgFuHQiOXFBeGuWBRFb9p1MJwIpLdxgwAd292998Fj7uALUAdsBa4O+h2N3BN8HgtcI8nPQlUmNkc4Epgg7u3uXs7sAFYM6GfJkusXlLD1gNdtHT1ZroUEZFRndQcgJktBM4HngJmuXszJEMCmBl0qwP2prysKWgbrT3nrD4juSyE9gJEJJuNOwDMrBT4V+CT7n6iK6BbmjY/QfvxP2e9mW0ys02tra3jLS+rrJhbTkW0UPMAIpLVxhUAZlZI8sv/n9z9x0HzgWBoh+C+JWhvAualvLwe2HeC9mHc/Q53b3D3htra2pP5LFkjHDIuOqOaJxoPanloEcla4zkKyIA7gS3u/tWUTQ8AQ0fyrAPuT2n/YHA00CrgcDBE9DBwhZlVBpO/VwRtOWn1khqaD/ey42B3pksREUmrYBx9VgN/BLxoZs8FbZ8Dvgzca2Y3AnuA64JtDwFXA41AD3ADgLu3mdkXgI1Bv8+7e84eLL9qcTUAG3e2cUZtaYarEREZacwAcPfHST9+D3B5mv4O3DTKe90F3HUyBU5Xi2tiVMUibNzVzvUXzM90OSIiI+hM4EliZjQsqGTT7pzdyRGRaU4BMInevLCK3Yd6aOnU+QAikn0UAJPozYuqANi4qz3DlYiIjKQAmERvmFtOcWGIjbs0DCQi2UcBMIkKwyHOn6d5ABHJTgqASfbmRVVs3tc54voA7s4Pnt6j9YJEJGMUAJPszQsrSTg8u2f4dYIf23aQW378Il95+NUMVSYi+U4BMMnOn19JyGDTcfMAdzy2HYCfPPcabd3xTJQmInlOATDJSosKeMPcGcOOBHrptcM80XiIa1fW0zeQ4AdP78lghSKSrxQAU6BhYSXP7m0nPpAA4I7HdlBaVMCt717BxUtq+P5vd9M/mMhwlSKSbxQAU+DNC6vo7U/w8r7DNLX38NMXm3n/BfMoLy7khtUL2d/Zy3+8tD/TZYpInlEATIGGhckrX27a1c5dj+/CgBtWLwLg0rNmsrA6ynef2JnBCkUkHykApsDMsmIWVkf5xZYD/HDjHt517lzmVpQAEAoZ6y5ayLN7Onhub8cY7yQiMnEUAFOkYWEVT+1soyc+yEfeunjYtve+qZ7SogLtBYjIlFIATJE3B8NAb11aw4q55cO2lRUXcl1DPT99oZkDWjhORKaIAmCKXHJmLXUVJXzi8qVpt3/oooUMuvPXD20hkdBlJEVk8o3nimAyAebMKOGJmy8bdfuC6hifvPxMvvaLVykrLuALa99I8mqcIiKTQwGQRT5++RJ6+gf4h1/voKggzP/8/eUKARGZNAqALGJm3LxmGX39Ce58fCfFhSH+7MplmS5LRHKUAiDLmBm3vmsFfQMJvv3odpoP9/Lh1Yt4Y92MTJcmIjlGAZCFzIwvXvNGyooLuOe3u/jx717j3HkV/MGF83nXOXMpiYQzXaKI5IAxjwIys7vMrMXMXkppqzKzDWa2LbivDNrNzL5pZo1m9oKZrUx5zbqg/zYzWzc5Hyd3hELG565ezlO3vJ1b37WC7r4BPnPfC7z3//6G3v7BtK/ZuKuNLzy4mUNH+qa4WhGZjsZzGOj3gDXHtd0MPOLuS4FHgucAVwFLg9t64HZIBgZwK3AhcAFw61BoyInNiBZyw+pFbPjTS/jG9efx8r5O/vqhLSP67T7UzY3f28idj+/k7V/9NT959jXcdTipiIxuzABw98eA469puBa4O3h8N3BNSvs9nvQkUGFmc4ArgQ3u3ubu7cAGRoaKnICZsfa8Ov7rxYu457e7+dlLzce2dfcNsP6eZzAz7vpQAwtrYnzyR8/x4e9tZF/H0QxWLSLZ7FRPBJvl7s0Awf3MoL0O2JvSryloG619BDNbb2abzGxTa2vrKZaXuz6zZhnn1s/gM/e9wN62HtydP7vveba1dHHbB87nsmWzuO+/XcRfvHMFT+5o48qvP8aO1iOZLltEstBEnwmc7qB1P0H7yEb3O9y9wd0bamtrJ7S4XBApCPGt96/EHT7+w2e57ZeNPPTifj67ZhlvXZr88wqHjA9fvIj/+MRbCZnxpz96TtcbEJERTjUADgRDOwT3LUF7EzAvpV89sO8E7XIK5ldH+dK1Z/Psng6+suFV3nnOHNZfsnhEv4U1Mb70nrN5vukw33pkWwYqFZFsdqoB8AAwdCTPOuD+lPYPBkcDrQIOB0NEDwNXmFllMPl7RdAmp+id58zlo7+3mFWLq/g/7z1n1DOGrz57DteurOe2RxtHXJdYRPKbjXWkiJn9AHgbUAMcIHk0z0+Ae4H5wB7gOndvs+S30G0kJ3h7gBvcfVPwPh8GPhe87Rfd/btjFdfQ0OCbNm06hY8lqbp6+7n6m/8JwEMffytlxYUZrkhEJpOZPePuDWP2y+ZDBRUAE2fTrjbe9w+/5T0r6/m7687NdDkiMonGGwBaDjpPNCys4qZLl3DfM0383cNbdY6AiGgpiHzyicuX0tLZx22PNrKv4yhfvvYcIgX6P4BIvlIA5JGCcIgvX3s29ZUlfGXDq+zv7OX2P3wTM0o0JyCSjxQAecbM+NjlS6mrLEmuLXT7b3hfwzwWVEdZWBNjflWU4kItNieSDxQAeeo9K+uZXV7Mp+59ni+mrC0UMvjw6kXcfNUyCsLDh4fau+P85b+/zJmzyrjp0iVTXbKITDAFQB67aEkNT37ucjp64uw+1MOuQ908vu0g33l8J5ubO7ntAyupikUAeHZPO3/yz8/yWrC20OzyYq59U30myxeR06QZQKEiGuHceRWsPa+Ov73uXP7uunPZtLudd33rcV567TB3/2YX7/uH32IGP/7ji7jojGpu+fGL/G5Pe6ZLF5HToPMAJK0Xmjr46Pef4UBnLwmHty+fyVeuO48Z0ULau+Os/fYTHO0f5N//5GJmzyjOdLkikkLnAchpOae+gn//2MVcffYc/vzq5dzxRw3MiCaPFqqMRfjHDzbQ0zfAR7+/adQL1IhIdlMAyKhqSou47QMr+cgliwmFhq81dNbsMr5+/fm88NphPv0vzzOYyN49SRFJTwEgp+wdK2bxuauW89MXmvn0vc8pBESmGR0FJKflI5csJj6Y4G8f3grAV953HuGUvYXtrUd4rf0ol5ypazuIZBsFgJy2oXMC/vbhrZgZX772bB59pZXvP7mLJxoPAfDf33YGn7nyrFGXrRaRqacAkAmRGgI/f3k/3fFB5s4o5n9ccSavdfRy+6+2c7Crjy+95+wRJ5iJSGYoAGTC3HTpEooKQjy54xDva5jHZctmUhAO4e7MKi/i67/YRlt3nNs+sJKSyPDlJtydgYQzMJicRzh+u4hMPJ0HIFPm/z25m/91/0ssrolRHSuivSdOe0+cjp5+Bo6bQL5s2UxuuvQM3rSgKkPVikxf4z0PQHsAMmX+cNUCakoj3P6r7ZjBkpmlVEQjVEQLiYRDFISMgnCIzt5+fvj0Hq69/bdcuKiKP750CbPKizjQ2ceBw720dPWyoDrG751VS3maq5t19vYzMOjHlrE4ftvTO9ro7O3nnefM1XLYkte0ByBZqSc+wA+e3ss/PraD/Z29afsUho1Vi6t5+/JZRApCPLunnWf3dNDYegT35HkMy+eUcdasMsJh48nth3jxtcMM7WwsnVnKF655I6sWV0/hJxOZfLokpOSEvoFBHtnSgjvMKi9iVnkxNaVFvLzvMBu2HGDD5gPsaO0GoDJayHnzKjh/fiXRSJhX9nfxyv5OXj1whETCOX9+BW9ZXM1bzqihq7efzz+4mab2o7xnZR23XLUcM2ju6GXf4aN09MRZVFPK8jllk3IN5fbuOAeP9FFaXEBpUQGxSMGIk+1ETpUCQPLGroPJAFhQHU17mOnAYIKBhI+4zsHR+CDf+uU2/vE/d9A/OPq/g4XVUVbMLWdWeTGV0QiV0UJmRCPEBxJ09fbT1TvAkb4BigvD1JYVMbOsiNqyIsJmdAbbO4/2s7e9hy3NXWxp7qT58Mi9mhklhcyvijK/Ksq8qih1FcUUhkOEzDCDosIwFy6qYla51l6SE8vaADCzNcA3gDDwHXf/8mh9FQAyFRpbunjg+WaqooXMqShhzoxiKkoiNLZ28fJrnby8r5NX9ndy8EicI30Dad+jqCBEfDDBif45hUPGktrkXsWKueXMnlFCT98AXb0DdPUN0Nbdx562o+xt66GpvWfUUDqnfgZvXz6Lt51VS3wgwZ62HnYf6qGp/Sh9A8PXZSotKqC6NEJVrIia0ggV0QjlxQWUlxRSXlyIGbR1x2nrjtPeHWd/Zy+7D/Ww82A3uw91c/hoP2fUlnLm7DKWzS5j6cwyZs8oZmZZEbGi8U0htnT28nJzJ5v3dbK5uZO+/gSxojDRSAGxSJgFNTEuWVrDgurYuN5PxpaVAWBmYeBV4B1AE7AReL+7b07XXwEg2SY+kKDjaJzDPf1ECkKUFRdSVlxAYTjEwGCCtu44LV19tHb1kXCnvCS5vby4kOrSCEUF4zu8dTDhHOruYzDhJBwSCefw0X5+/Worv9hygOf2dgwLG7PkNRqGHT7rBMESP6llOkqLCo5dIa68uJDtLUd4ZX8nnb3Dwy8WCVNTVkTIjIR78pZI1j6QcAYTCfoHfVhozqsqoayokJ74AN3xQbr7BuiJJ0NrQXWUS5bWsnxOOYPuDAZ7bkfjgxwKQupQdx+dRwdw/NjnN4NoYQGxojCxomBIrSgZLrGiAkoiYbp6B2jt6uNQd5xDR/qIFISojhVRXRqhOhahMhahMhqhKlZIRTQqRzUvAAAG/UlEQVRCUUGIgcHg0OREgoFBpz+op38wMaymtu44vf2DLKqJsXRmGUtnlTKzrGjUkx4TCaenf5CQQWFw8IOZ0T+YoKOnn46eOO09/ZQWFbBibvm4f2+psjUA3gL8pbtfGTy/BcDdv5SuvwJAJL2Wrl6e3NFGeXEB86ui1FWWjBouiYTT2dvPwSNxDh+N03l0gM7efjqP9uNAVSxCVTT5JVhbVkR1LDLiy8vd2d/ZS2PLEQ509tHS1UtLZ/ILFZJXkguZETKjIGSEw8F9yJhXGeUNc8tZPrd8xFFb7s6uQz089morj73aym+2H+JomtVly4oKqAq+rGeUFBJKqW/QnZ74YDJU+gY50jdAT18yYI5/j5qyIiqjhcQHE7QdiXOwO058IHEqv4JjigpCRMIhulKCLhoJE42EKQyHjn3J98STtR2/F2lBEBxfx++fM4dvf2DlKdWUrYeB1gF7U543ARdOcQ0i097MsmLefe7ccfUNhSw43HbkYbHjZWbMmVHCnBklp/weo73vopoYi2pirLtoIX0Dg7R39xMOvR4kRQWhce85pUoknKP9g/TEBykrLkh7rWt3pzs+SHt38pyUtuA+PpCgIBSiIGzHvsCPfZmHjeLCMNWxCFWxCNFgr+vgkTjbDnSxreUIuw/10DcwSHwgQf9ggv6EEy0MU1ZcSGlxcu/Egf6BBPHB5C1aWEBlsAdSGS2kvjJ6un+8Y5rqAEi3TzRsF8TM1gPrAebPnz8VNYlIligqCDN7xsScBR4KWXIo6ARzFWZGaTBsNK/q9L5wa4PJ/4uW1JzW+0ylqT4LpgmYl/K8HtiX2sHd73D3BndvqK3VCpIiIpNlqgNgI7DUzBaZWQS4HnhgimsQERGmeAjI3QfM7E+Ah0keBnqXu788lTWIiEjSlK8F5O4PAQ9N9c8VEZHhtBKWiEieUgCIiOQpBYCISJ5SAIiI5KmsXg3UzFqB3afxFjXAwQkqZ7KoxomhGieGapwYma5xgbuPeSJVVgfA6TKzTeNZDyOTVOPEUI0TQzVOjOlQI2gISEQkbykARETyVK4HwB2ZLmAcVOPEUI0TQzVOjOlQY27PAYiIyOhyfQ9ARERGkZMBYGZrzGyrmTWa2c2ZrgfAzO4ysxYzeymlrcrMNpjZtuC+MsM1zjOzR81si5m9bGafyLY6zazYzJ42s+eDGv8qaF9kZk8FNf4oWG02o8wsbGbPmtmDWVzjLjN70cyeM7NNQVvW/L6DeirM7D4zeyX4u/mWbKrRzM4K/vyGbp1m9slsqnE0ORcAwXWHvw1cBawA3m9mKzJbFQDfA9Yc13Yz8Ii7LwUeCZ5n0gDwaXdfDqwCbgr+7LKpzj7gMnc/FzgPWGNmq4C/Ab4W1NgO3JjBGod8AtiS8jwbawS41N3PSzlsMZt+3wDfAH7m7suAc0n+mWZNje6+NfjzOw94E9AD/Fs21Tgqd8+pG/AW4OGU57cAt2S6rqCWhcBLKc+3AnOCx3OArZmu8bh67wfeka11AlHgdyQvK3oQKEj3dyBDtdWT/Ed/GfAgyavhZVWNQR27gJrj2rLm9w2UAzsJ5iuzscbj6roCeCKba0y95dweAOmvO1yXoVrGMsvdmwGC+5kZrucYM1sInA88RZbVGQytPAe0ABuA7UCHuw9dbTsbfudfBz4DDF3pu5rsqxGSl2T9uZk9E1yOFbLr970YaAW+GwynfcfMYllWY6rrgR8Ej7O1xmNyMQDGvO6wnJiZlQL/CnzS3TszXc/x3H3Qk7vb9cAFwPJ03aa2qteZ2TuBFnd/JrU5Tdds+Hu52t1XkhwyvcnMLsl0QccpAFYCt7v7+UA32TiUAgRzOu8G/iXTtYxXLgbAmNcdziIHzGwOQHDfkuF6MLNCkl/+/+TuPw6as65OAHfvAH5Fcr6iwsyGLnCU6d/5auDdZrYL+CHJYaCvk101AuDu+4L7FpLj1heQXb/vJqDJ3Z8Knt9HMhCyqcYhVwG/c/cDwfNsrHGYXAyA6XTd4QeAdcHjdSTH3DPGzAy4E9ji7l9N2ZQ1dZpZrZlVBI9LgLeTnBR8FHhv0C2jNbr7Le5e7+4LSf79+6W7/wFZVCOAmcXMrGzoMcnx65fIot+3u+8H9prZWUHT5cBmsqjGFO/n9eEfyM4ah8v0JMQkTcRcDbxKcmz4zzNdT1DTD4BmoJ/k/2puJDku/AiwLbivynCNF5MclngBeC64XZ1NdQLnAM8GNb4E/EXQvhh4GmgkuQtelOnfeVDX24AHs7HGoJ7ng9vLQ/9Wsun3HdRzHrAp+J3/BKjMwhqjwCFgRkpbVtWY7qYzgUVE8lQuDgGJiMg4KABERPKUAkBEJE8pAERE8pQCQEQkTykARETylAJARCRPKQBERPLU/wfNYHiQKUJ/9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn = BNNBayesbyBackprop(prior_mu=0.0, prior_s=1.0, num_MC_samples=30, linear_regression=True, preset=False, classification=True, input_dim=2)\n",
    "bnn.fit(X_train, y_train, plot=True, n_epochs=75, learning_rate=1e-2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. ]\n",
      " [0.3]]\n",
      "0.1\n",
      "[[  9.95293375]\n",
      " [ 38.79069985]\n",
      " [-65.24593269]\n",
      " ...\n",
      " [-79.93915925]\n",
      " [-18.69534254]\n",
      " [-74.1674691 ]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([-10, 10]).reshape(-1, 1)\n",
    "b = -5\n",
    "X_train = np.random.uniform(-10,10, (5000, 2))\n",
    "X_train[:,0] = 0.0\n",
    "X_train.shape\n",
    "W.T.shape\n",
    "\n",
    "eps = 0.0001\n",
    "W_s = np.array([0.0, 0.3]).reshape(-1, 1)\n",
    "print(W_s)\n",
    "b_s = 0.1\n",
    "\n",
    "print(b_s)\n",
    "\n",
    "y_train = np.zeros((X_train.shape[0], 1), dtype=float)\n",
    "for ind, x in enumerate(X_train):\n",
    "  # y_train[ind] = x @ W + b + np.random.normal(loc=0, scale=np.exp(x @ W_s + b_s))\n",
    "  y_train[ind] = x.reshape(1,-1) @ W + b + np.random.normal(loc=0, scale=np.exp(x @ W_s + b_s))\n",
    "\n",
    "\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  2\n",
      "Data being saved in following file:\n",
      "logging.csv\n",
      "Epoch:  0 \tLoss:  855162000.0 \tMAE:  49.99776\n",
      "Epoch:  1 \tLoss:  870236500.0 \tMAE:  53.1579\n",
      "Epoch:  2 \tLoss:  868457400.0 \tMAE:  50.17365\n",
      "Epoch:  3 \tLoss:  846506900.0 \tMAE:  45.173042\n",
      "Epoch:  4 \tLoss:  858328770.0 \tMAE:  60.033108\n",
      "Epoch:  5 \tLoss:  863716700.0 \tMAE:  49.275055\n",
      "Epoch:  6 \tLoss:  850691460.0 \tMAE:  50.327698\n",
      "Epoch:  7 \tLoss:  830452200.0 \tMAE:  48.522823\n",
      "Epoch:  8 \tLoss:  848222100.0 \tMAE:  48.067677\n",
      "Epoch:  9 \tLoss:  863312000.0 \tMAE:  49.250755\n",
      "Epoch:  10 \tLoss:  866392640.0 \tMAE:  55.357388\n",
      "Epoch:  11 \tLoss:  839693060.0 \tMAE:  48.596806\n",
      "Epoch:  12 \tLoss:  829836350.0 \tMAE:  48.042908\n",
      "Epoch:  13 \tLoss:  867969900.0 \tMAE:  47.748524\n",
      "Epoch:  14 \tLoss:  835277600.0 \tMAE:  43.61436\n",
      "Epoch:  15 \tLoss:  23102350000.0 \tMAE:  51.690266\n",
      "Epoch:  16 \tLoss:  21817364000.0 \tMAE:  57.230686\n",
      "Epoch:  17 \tLoss:  22883805000.0 \tMAE:  45.5243\n",
      "Epoch:  18 \tLoss:  23456514000.0 \tMAE:  52.84144\n",
      "Epoch:  19 \tLoss:  21822583000.0 \tMAE:  51.11709\n",
      "Epoch:  20 \tLoss:  2.6147557e+23 \tMAE:  50.726097\n",
      "Epoch:  21 \tLoss:  9.704897e+21 \tMAE:  51.638523\n",
      "Epoch:  22 \tLoss:  4.7272067e+24 \tMAE:  46.445198\n",
      "Epoch:  23 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  24 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  25 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  26 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  27 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  28 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  29 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  30 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  31 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  32 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  33 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  34 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  35 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  36 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  37 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  38 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  39 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  40 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  41 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  42 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  43 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  44 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  45 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  46 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  47 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  48 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  49 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  50 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  51 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  52 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  53 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  54 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  55 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  56 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  57 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  58 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  59 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  60 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  61 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  62 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  63 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  64 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  65 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  66 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  67 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  68 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  69 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  70 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  71 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  72 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  73 \tLoss:  nan \tMAE:  nan\n",
      "Epoch:  74 \tLoss:  nan \tMAE:  nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnn = BNNBayesbyBackprop(prior_mu=0.0, prior_s=1.0, num_MC_samples=30, linear_regression=True, preset=False, classification=False, input_dim=2)\n",
    "bnn.fit(X_train, y_train, plot=True, n_epochs=75, learning_rate=1e-3, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
